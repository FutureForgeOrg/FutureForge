name: Automated Job Scraping

on:
  # Schedule: 6 times per month at 6:30 AM IST (1:00 AM UTC)
  schedule:
    - cron: "0 1 1,5,10,15,20,25 * *" # 1st,5th, 10th, 15th, 20th, 25th of each month

  # Allow manual trigger
  workflow_dispatch:
    inputs:
      max_jobs:
        description: "Maximum jobs per role"
        required: false
        default: "15"
        type: string
      test_mode:
        description: "Run in test mode (first 3 roles only)"
        required: false
        default: false
        type: boolean

jobs:
  scrape-jobs:
    runs-on: ubuntu-latest
    timeout-minutes: 30 # Prevent hanging jobs

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.12"
          cache: "pip"

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r services/job_engine/requirements.txt

      - name: Validate environment
        env:
          SERPAPI_API_KEY: ${{ secrets.SERPAPI_API_KEY }}
          MONGODB_URI: ${{ secrets.MONGODB_URI }}
          DATABASE_NAME: ${{ secrets.DATABASE_NAME }}
          JOBS_COLLECTION: ${{ secrets.JOBS_COLLECTION }}
          LOCATION_MODE: India
          GITHUB_ACTIONS_MODE: true
          SCRAPING_ENABLED: true
        run: |
          echo "Validating scraping environment..."
          python -c "
          import sys
          sys.path.append('services/job_engine')
          from common.config import config
          print(f'Location Mode: {config.LOCATION_MODE}')
          print(f'Job Roles: {len(config.get_job_roles())} configured')
          print(f'GitHub Actions Mode: {config.GITHUB_ACTIONS_MODE}')
          print('Environment validation successful')
          "

      - name: Run job scraping
        env:
          SERPAPI_API_KEY: ${{ secrets.SERPAPI_API_KEY }}
          MONGODB_URI: ${{ secrets.MONGODB_URI }}
          DATABASE_NAME: ${{ secrets.DATABASE_NAME }}
          JOBS_COLLECTION: ${{ secrets.JOBS_COLLECTION }}
          SCRAPING_ENABLED: true
          LOCATION_MODE: India
          GITHUB_ACTIONS_MODE: true
          MAX_RESULTS_PER_QUERY: 20
          API_TIMEOUT: 30
          REQUEST_DELAY: 2
          MAX_JOBS_PER_ROLE: ${{ github.event.inputs.max_jobs || '15' }}
          TEST_MODE: ${{ github.event.inputs.test_mode || 'false' }}
          SAVE_RESULTS: false
        run: |
          cd services/job_engine
          python scripts/github_actions_runner.py

      - name: Upload results (if save-results flag used)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraping-results-${{ github.run_number }}
          path: services/job_engine/scraping_results_*.json
          if-no-files-found: ignore
          retention-days: 7

      - name: Clear Redis job cache
        env:
          REDIS_HOST: ${{ secrets.REDIS_HOST }}
          REDIS_PORT: ${{ secrets.REDIS_PORT }}
          REDIS_PASSWORD: ${{ secrets.REDIS_PASSWORD }}
        run: |
          pip install redis
          python -c "
          import redis
          r = redis.Redis(
              host='${{ secrets.REDIS_HOST }}',
              port=int('${{ secrets.REDIS_PORT }}'),
              username='default',
              password='${{ secrets.REDIS_PASSWORD }}',
              decode_responses=True
          )
          deleted = r.delete('latest_jobs_static')
          print(f'Redis key cleared: latest_jobs_static | Deleted: {deleted}')
          "

      - name: Notify on failure
        if: failure()
        run: |
          echo "Job scraping failed! Check the logs above for details."
          echo "Common issues:"
          echo "1. API key expired or invalid"
          echo "2. Database connection issues"
          echo "3. Rate limiting from SerpAPI"
          echo "4. Network connectivity issues"
